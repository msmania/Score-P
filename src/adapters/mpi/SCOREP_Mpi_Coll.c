/*
 * This file is part of the Score-P software (http://www.score-p.org)
 *
 * Copyright (c) 2009-2013,
 * RWTH Aachen University, Germany
 *
 * Copyright (c) 2009-2013,
 * Gesellschaft fuer numerische Simulation mbH Braunschweig, Germany
 *
 * Copyright (c) 2009-2017,
 * Technische Universitaet Dresden, Germany
 *
 * Copyright (c) 2009-2013,
 * University of Oregon, Eugene, USA
 *
 * Copyright (c) 2009-2015, 2019, 2022,
 * Forschungszentrum Juelich GmbH, Germany
 *
 * Copyright (c) 2009-2014,
 * German Research School for Simulation Sciences GmbH, Juelich/Aachen, Germany
 *
 * Copyright (c) 2009-2013,
 * Technische Universitaet Muenchen, Germany
 *
 * Copyright (c) 2016,
 * Technische Universitaet Darmstadt, Germany
 *
 * Copyright (c) 2022,
 * Deutsches Zentrum fuer Luft- und Raumfahrt, Germany
 *
 * This software may be modified and distributed under the terms of
 * a BSD-style license.  See the COPYING file in the package base
 * directory for details.
 *
 */

/****************************************************************************
**  SCALASCA    http://www.scalasca.org/                                   **
*****************************************************************************
**  Copyright (c) 1998-2011                                                **
**  Forschungszentrum Juelich GmbH, Juelich Supercomputing Centre          **
**                                                                         **
**  Copyright (c) 2010-2011                                                **
**  German Research School for Simulation Sciences GmbH,                   **
**  Laboratory for Parallel Programming                                    **
**                                                                         **
**  Copyright (c) 2003-2008                                                **
**  University of Tennessee, Innovative Computing Laboratory               **
**                                                                         **
**  See the file COPYRIGHT in the package base directory for details       **
****************************************************************************/


/**
 * @file
 * @ingroup    MPI_Wrapper
 *
 * @brief C interface wrappers for collective communication
 */

#include <config.h>
#include "SCOREP_Mpi.h"
#include "scorep_mpi_communicator.h"
#include "scorep_mpi_request_mgmt.h"
#include <SCOREP_RuntimeManagement.h>
#include <SCOREP_InMeasurement.h>
#include <SCOREP_Events.h>

/**
 * @name C wrappers
 * @{
 */

#if HAVE( MPI_1_0_SYMBOL_PMPI_ALLGATHER )
/**
 * Declaration of PMPI-symbol for MPI_Allgather
 */
int
PMPI_Allgather( SCOREP_MPI_CONST_DECL void* sendbuf,
                int                         sendcount,
                MPI_Datatype                sendtype,
                void*                       recvbuf,
                int                         recvcount,
                MPI_Datatype                recvtype,
                MPI_Comm                    comm );

/**
 * Measurement wrapper for MPI_Allgather
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-1.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Allgather( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    int            recvsz, sendsz, N, M;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Comm_size( comm, &N );
            PMPI_Type_size( recvtype, &recvsz );

            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                if ( sendbuf != MPI_IN_PLACE )
                {
                    PMPI_Type_size( sendtype, &sendsz );
                    sendbytes = ( uint64_t )N * sendcount * sendsz;
                    recvbytes = ( uint64_t )N * recvcount * recvsz;
                }
                else
                {
                    sendbytes = recvbytes = ( uint64_t )( N - 1 ) * recvcount * recvsz;
                }
            }
            else
            {
                PMPI_Comm_remote_size( comm, &M );
                PMPI_Type_size( sendtype, &sendsz );

                sendbytes = ( uint64_t )M * sendcount * sendsz;
                recvbytes = ( uint64_t )M * recvcount * recvsz;
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLGATHER ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLGATHER ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Allgather( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_ALLGATHER,
                                     sendbytes,
                                     recvbytes );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLGATHER ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLGATHER ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_1_0_SYMBOL_PMPI_ALLGATHERV ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Allgatherv )
/**
 * Declaration of PMPI-symbol for MPI_Allgatherv
 */
int
PMPI_Allgatherv( SCOREP_MPI_CONST_DECL void* sendbuf,
                 int                         sendcount,
                 MPI_Datatype                sendtype,
                 void*                       recvbuf,
                 SCOREP_MPI_CONST_DECL int*  recvcounts,
                 SCOREP_MPI_CONST_DECL int*  displs,
                 MPI_Datatype                recvtype,
                 MPI_Comm                    comm );

/**
 * Measurement wrapper for MPI_Allgatherv
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-1.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Allgatherv( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, SCOREP_MPI_CONST_DECL int* recvcounts, SCOREP_MPI_CONST_DECL int* displs, MPI_Datatype recvtype, MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    int32_t        recvcount = 0, recvsz, sendsz, i, N, M, me;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Comm_size( comm, &N );
            PMPI_Type_size( recvtype, &recvsz );

            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                PMPI_Comm_rank( comm, &me );

                if ( sendbuf != MPI_IN_PLACE )
                {
                    PMPI_Type_size( sendtype, &sendsz );
                    sendbytes = ( uint64_t )N * sendcount * sendsz;
                }
                else
                {
                    sendbytes = ( uint64_t )( N - 1 ) * recvcounts[ me ] * recvsz;
                }

                for ( i = 0; i < N; i++ )
                {
                    recvcount += recvcounts[ i ];
                }

                if ( sendbuf == MPI_IN_PLACE )
                {
                    recvcount -= recvcounts[ me ];
                }
                recvbytes = ( uint64_t )recvcount * recvsz;
            }
            else
            {
                PMPI_Comm_remote_size( comm, &M );
                PMPI_Type_size( sendtype, &sendsz );

                for ( i = 0; i < M; i++ )
                {
                    recvcount += recvcounts[ i ];
                }

                sendbytes = ( uint64_t )M * sendcount * sendsz;
                recvbytes = ( uint64_t )recvcount * recvsz;
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLGATHERV ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLGATHERV ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Allgatherv( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_ALLGATHERV,
                                     sendbytes,
                                     recvbytes );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLGATHERV ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLGATHERV ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_1_0_SYMBOL_PMPI_ALLREDUCE ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Allreduce )
/**
 * Declaration of PMPI-symbol for MPI_Allreduce
 */
int
PMPI_Allreduce( SCOREP_MPI_CONST_DECL void* sendbuf,
                void*                       recvbuf,
                int                         count,
                MPI_Datatype                datatype,
                MPI_Op                      op,
                MPI_Comm                    comm );

/**
 * Measurement wrapper for MPI_Allreduce
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-1.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Allreduce( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    int32_t        sz, N;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Type_size( datatype, &sz );

            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                PMPI_Comm_size( comm, &N );
                if ( sendbuf == MPI_IN_PLACE )
                {
                    N--;
                }
            }
            else
            {
                PMPI_Comm_remote_size( comm, &N );
            }

            sendbytes = recvbytes = ( uint64_t )N * count * sz;


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLREDUCE ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLREDUCE ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Allreduce( sendbuf, recvbuf, count, datatype, op, comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_ALLREDUCE,
                                     sendbytes,
                                     recvbytes );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLREDUCE ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLREDUCE ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_1_0_SYMBOL_PMPI_ALLTOALL ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Alltoall )
/**
 * Declaration of PMPI-symbol for MPI_Alltoall
 */
int
PMPI_Alltoall( SCOREP_MPI_CONST_DECL void* sendbuf,
               int                         sendcount,
               MPI_Datatype                sendtype,
               void*                       recvbuf,
               int                         recvcount,
               MPI_Datatype                recvtype,
               MPI_Comm                    comm );

/**
 * Measurement wrapper for MPI_Alltoall
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-1.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Alltoall( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    int32_t        recvsz, sendsz, N;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                PMPI_Type_size( recvtype, &recvsz );
                PMPI_Comm_size( comm, &N );

                if ( sendbuf == MPI_IN_PLACE )
                {
                    --N;
                }

                sendbytes = ( uint64_t )N * recvcount * recvsz;
                recvbytes = sendbytes;
            }
            else
            {
                PMPI_Comm_remote_size( comm, &N );
                PMPI_Type_size( recvtype, &recvsz );
                PMPI_Type_size( sendtype, &sendsz );

                sendbytes = ( uint64_t )N * sendcount * sendsz;
                recvbytes = ( uint64_t )N * recvcount * recvsz;
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALL ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALL ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Alltoall( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_ALLTOALL,
                                     sendbytes,
                                     recvbytes );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALL ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALL ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_1_0_SYMBOL_PMPI_ALLTOALLV ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Alltoallv )
/**
 * Declaration of PMPI-symbol for MPI_Alltoallv
 */
int
PMPI_Alltoallv( SCOREP_MPI_CONST_DECL void* sendbuf,
                SCOREP_MPI_CONST_DECL int*  sendcounts,
                SCOREP_MPI_CONST_DECL int*  sdispls,
                MPI_Datatype                sendtype,
                void*                       recvbuf,
                SCOREP_MPI_CONST_DECL int*  recvcounts,
                SCOREP_MPI_CONST_DECL int*  rdispls,
                MPI_Datatype                recvtype,
                MPI_Comm                    comm );

/**
 * Measurement wrapper for MPI_Alltoallv
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-1.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Alltoallv( SCOREP_MPI_CONST_DECL void* sendbuf, SCOREP_MPI_CONST_DECL int* sendcounts, SCOREP_MPI_CONST_DECL int* sdispls, MPI_Datatype sendtype, void* recvbuf, SCOREP_MPI_CONST_DECL int* recvcounts, SCOREP_MPI_CONST_DECL int* rdispls, MPI_Datatype recvtype, MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    int            recvcount = 0, recvsz, sendsz, N, i, me;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                PMPI_Comm_size( comm, &N );
                PMPI_Type_size( recvtype, &recvsz );

                if ( sendbuf != MPI_IN_PLACE )
                {
                    PMPI_Type_size( sendtype, &sendsz );
                    for ( i = 0; i < N; i++ )
                    {
                        recvbytes += ( uint64_t )recvcounts[ i ] * recvsz;
                        sendbytes += ( uint64_t )sendcounts[ i ] * sendsz;
                    }
                }
                else
                {
                    PMPI_Comm_rank( comm, &me );
                    for ( i = 0; i < N; i++ )
                    {
                        recvcount += recvcounts[ i ];
                    }

                    recvcount -= recvcounts[ me ];

                    sendbytes = recvbytes = ( uint64_t )recvcount * recvsz;
                }
            }
            else
            {
                PMPI_Comm_remote_size( comm, &N );
                PMPI_Type_size( recvtype, &recvsz );
                PMPI_Type_size( sendtype, &sendsz );

                for ( i = 0; i < N; i++ )
                {
                    recvbytes += ( uint64_t )recvcounts[ i ] * recvsz;
                    sendbytes += ( uint64_t )sendcounts[ i ] * sendsz;
                }
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALLV ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALLV ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Alltoallv( sendbuf, sendcounts, sdispls, sendtype, recvbuf, recvcounts, rdispls, recvtype, comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_ALLTOALLV,
                                     sendbytes,
                                     recvbytes );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALLV ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALLV ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_2_0_SYMBOL_PMPI_ALLTOALLW ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Alltoallw )
/**
 * Declaration of PMPI-symbol for MPI_Alltoallw
 */
int
PMPI_Alltoallw( SCOREP_MPI_CONST_DECL void*        sendbuf,
                SCOREP_MPI_CONST_DECL int          sendcounts[],
                SCOREP_MPI_CONST_DECL int          sdispls[],
                SCOREP_MPI_CONST_DECL MPI_Datatype sendtypes[],
                void*                              recvbuf,
                SCOREP_MPI_CONST_DECL int          recvcounts[],
                SCOREP_MPI_CONST_DECL int          rdispls[],
                SCOREP_MPI_CONST_DECL MPI_Datatype recvtypes[],
                MPI_Comm                           comm );

/**
 * Measurement wrapper for MPI_Alltoallw
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-2.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Alltoallw( SCOREP_MPI_CONST_DECL void* sendbuf, SCOREP_MPI_CONST_DECL int sendcounts[], SCOREP_MPI_CONST_DECL int sdispls[], SCOREP_MPI_CONST_DECL MPI_Datatype sendtypes[], void* recvbuf, SCOREP_MPI_CONST_DECL int recvcounts[], SCOREP_MPI_CONST_DECL int rdispls[], SCOREP_MPI_CONST_DECL MPI_Datatype recvtypes[], MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    int            recvsz, sendsz, N, i, me;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                PMPI_Comm_size( comm, &N );

                if ( sendbuf != MPI_IN_PLACE )
                {
                    for ( i = 0; i < N; i++ )
                    {
                        PMPI_Type_size( recvtypes[ i ], &recvsz );
                        recvbytes += ( uint64_t )recvcounts[ i ] * recvsz;

                        PMPI_Type_size( sendtypes[ i ], &sendsz );
                        sendbytes += ( uint64_t )sendcounts[ i ] * sendsz;
                    }
                }
                else
                {
                    PMPI_Comm_rank( comm, &me );

                    for ( i = 0; i < N; i++ )
                    {
                        PMPI_Type_size( recvtypes[ i ], &recvsz );
                        recvbytes += ( uint64_t )recvcounts[ i ] * recvsz;
                    }

                    PMPI_Type_size( recvtypes[ me ], &recvsz );
                    recvbytes -= ( uint64_t )recvcounts[ me ] * recvsz;

                    sendbytes = recvbytes;
                }
            }
            else
            {
                PMPI_Comm_remote_size( comm, &N );

                for ( i = 0; i < N; i++ )
                {
                    PMPI_Type_size( recvtypes[ i ], &recvsz );
                    PMPI_Type_size( sendtypes[ i ], &sendsz );
                    recvbytes += ( uint64_t )recvcounts[ i ] * recvsz;
                    sendbytes += ( uint64_t )sendcounts[ i ] * sendsz;
                }
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALLW ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALLW ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Alltoallw( sendbuf, sendcounts, sdispls, sendtypes, recvbuf, recvcounts, rdispls, recvtypes, comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_ALLTOALLW,
                                     sendbytes,
                                     recvbytes );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALLW ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALLW ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_1_0_SYMBOL_PMPI_BARRIER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Barrier )
/**
 * Declaration of PMPI-symbol for MPI_Barrier
 */
int
PMPI_Barrier( MPI_Comm comm );

/**
 * Measurement wrapper for MPI_Barrier
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-1.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Barrier( MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int      event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int      event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int            return_val;
    SCOREP_MpiRank root_loc = SCOREP_INVALID_ROOT_RANK;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_BARRIER ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_BARRIER ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Barrier( comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_BARRIER,
                                     0,
                                     0 );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_BARRIER ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_BARRIER ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_1_0_SYMBOL_PMPI_BCAST ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Bcast )
/**
 * Declaration of PMPI-symbol for MPI_Bcast
 */
int
PMPI_Bcast( void*        buffer,
            int          count,
            MPI_Datatype datatype,
            int          root,
            MPI_Comm     comm );

/**
 * Measurement wrapper for MPI_Bcast
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-1.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Bcast( void* buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    int32_t        sz, N, me;
    int32_t        participant = 1;
    uint64_t       sendbytes   = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc    = scorep_mpi_get_scorep_mpi_rank( root );


    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Type_size( datatype, &sz );

            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                PMPI_Comm_rank( comm, &me );
                if ( me == root )
                {
                    PMPI_Comm_size( comm, &N );
                }
                else
                {
                    N = 0;
                }
            }
            else
            {
                if ( root == MPI_ROOT )
                {
                    PMPI_Comm_remote_size( comm, &N );
                    participant = 0;
                }
                else if ( root == MPI_PROC_NULL )
                {
                    participant = 0;
                    N           = 0;
                }
                else
                {
                    N = 0;
                }
            }

            sendbytes = ( uint64_t )N * count * sz;
            recvbytes = ( uint64_t )participant * count * sz;


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_BCAST ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_BCAST ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Bcast( buffer, count, datatype, root, comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_BCAST,
                                     sendbytes,
                                     recvbytes );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_BCAST ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_BCAST ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_2_0_SYMBOL_PMPI_EXSCAN ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Exscan )
/**
 * Declaration of PMPI-symbol for MPI_Exscan
 */
int
PMPI_Exscan( SCOREP_MPI_CONST_DECL void* sendbuf,
             void*                       recvbuf,
             int                         count,
             MPI_Datatype                datatype,
             MPI_Op                      op,
             MPI_Comm                    comm );

/**
 * Measurement wrapper for MPI_Exscan
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-2.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Exscan( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    int32_t        sz, me, N;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Type_size( datatype, &sz );
            PMPI_Comm_rank( comm, &me );
            PMPI_Comm_size( comm, &N );

            sendbytes = ( uint64_t )( N - me - 1 ) * sz * count;
            recvbytes = ( uint64_t )me * sz * count;


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_EXSCAN ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_EXSCAN ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Exscan( sendbuf, recvbuf, count, datatype, op, comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_EXSCAN,
                                     sendbytes,
                                     recvbytes );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_EXSCAN ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_EXSCAN ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_1_0_SYMBOL_PMPI_GATHER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Gather )
/**
 * Declaration of PMPI-symbol for MPI_Gather
 */
int
PMPI_Gather( SCOREP_MPI_CONST_DECL void* sendbuf,
             int                         sendcount,
             MPI_Datatype                sendtype,
             void*                       recvbuf,
             int                         recvcount,
             MPI_Datatype                recvtype,
             int                         root,
             MPI_Comm                    comm );

/**
 * Measurement wrapper for MPI_Gather
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-1.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Gather( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    int            sendsz, recvsz, N, me;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = scorep_mpi_get_scorep_mpi_rank( root );


    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                if ( sendbuf != MPI_IN_PLACE )
                {
                    PMPI_Type_size( sendtype, &sendsz );
                    sendbytes = ( uint64_t )sendcount * sendsz;
                }
                /* MPI_IN_PLACE: sendbytes is initialized to 0 */

                PMPI_Comm_rank( comm, &me );
                if ( me == root )
                {
                    PMPI_Comm_size( comm, &N );
                    PMPI_Type_size( recvtype, &recvsz );
                    if ( sendbuf == MPI_IN_PLACE )
                    {
                        --N;
                    }
                    recvbytes = ( uint64_t )N * recvcount * recvsz;
                }
            }
            else
            {
                if ( root == MPI_ROOT )
                {
                    PMPI_Comm_remote_size( comm, &N );
                    PMPI_Type_size( recvtype, &recvsz );
                    recvbytes = ( uint64_t )N * recvcount * recvsz;
                }
                else if ( root != MPI_PROC_NULL )
                {
                    PMPI_Type_size( sendtype, &sendsz );
                    sendbytes = ( uint64_t )sendcount * sendsz;
                }
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_GATHER ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_GATHER ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Gather( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_GATHER,
                                     sendbytes,
                                     recvbytes );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_GATHER ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_GATHER ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_1_0_SYMBOL_PMPI_GATHERV ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Gatherv )
/**
 * Declaration of PMPI-symbol for MPI_Gatherv
 */
int
PMPI_Gatherv( SCOREP_MPI_CONST_DECL void* sendbuf,
              int                         sendcount,
              MPI_Datatype                sendtype,
              void*                       recvbuf,
              SCOREP_MPI_CONST_DECL int*  recvcounts,
              SCOREP_MPI_CONST_DECL int*  displs,
              MPI_Datatype                recvtype,
              int                         root,
              MPI_Comm                    comm );

/**
 * Measurement wrapper for MPI_Gatherv
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-1.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Gatherv( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, SCOREP_MPI_CONST_DECL int* recvcounts, SCOREP_MPI_CONST_DECL int* displs, MPI_Datatype recvtype, int root, MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    int            recvsz, sendsz, me, N, i;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = scorep_mpi_get_scorep_mpi_rank( root );


    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                if ( sendbuf != MPI_IN_PLACE )
                {
                    PMPI_Type_size( sendtype, &sendsz );
                    sendbytes = ( uint64_t )sendcount * sendsz;
                }
                /* MPI_IN_PLACE: sendbytes is initialized to 0 */

                PMPI_Comm_rank( comm, &me );
                if ( me == root )
                {
                    PMPI_Comm_size( comm, &N );
                    PMPI_Type_size( recvtype, &recvsz );

                    for ( i = 0; i < N; ++i )
                    {
                        recvbytes += ( uint64_t )recvcounts[ i ] * recvsz;
                    }

                    if ( sendbuf == MPI_IN_PLACE )
                    {
                        recvbytes -= ( uint64_t )recvcounts[ me ] * recvsz;
                    }
                }
            }
            else
            {
                if ( root == MPI_ROOT )
                {
                    PMPI_Comm_remote_size( comm, &N );
                    PMPI_Type_size( recvtype, &recvsz );

                    for ( i = 0; i < N; ++i )
                    {
                        recvbytes += ( uint64_t )recvcounts[ i ] * recvsz;
                    }
                }
                else if ( root != MPI_PROC_NULL )
                {
                    PMPI_Type_size( sendtype, &sendsz );
                    sendbytes = ( uint64_t )sendcount * sendsz;
                }
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_GATHERV ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_GATHERV ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Gatherv( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, root, comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_GATHERV,
                                     sendbytes,
                                     recvbytes );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_GATHERV ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_GATHERV ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_1_0_SYMBOL_PMPI_REDUCE ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Reduce )
/**
 * Declaration of PMPI-symbol for MPI_Reduce
 */
int
PMPI_Reduce( SCOREP_MPI_CONST_DECL void* sendbuf,
             void*                       recvbuf,
             int                         count,
             MPI_Datatype                datatype,
             MPI_Op                      op,
             int                         root,
             MPI_Comm                    comm );

/**
 * Measurement wrapper for MPI_Reduce
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-1.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Reduce( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    int            sz, me, N;
    uint64_t       sendbytes = 0, recvbytes = 0, participant = 1;
    SCOREP_MpiRank root_loc  = scorep_mpi_get_scorep_mpi_rank( root );


    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Type_size( datatype, &sz );
            PMPI_Comm_rank( comm, &me );
            PMPI_Comm_size( comm, &N );

            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                if ( sendbuf != MPI_IN_PLACE )
                {
                    sendbytes = ( uint64_t )count * sz;
                }
                else
                {
                    --N;
                }

                if ( root == me )
                {
                    recvbytes = ( uint64_t )N * count * sz;
                }
            }
            else
            {
                if ( root == MPI_ROOT )
                {
                    PMPI_Comm_remote_size( comm, &N );
                    recvbytes = ( uint64_t )N * count * sz;
                }
                else if ( root != MPI_PROC_NULL )
                {
                    sendbytes = ( uint64_t )count * sz;
                }
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Reduce( sendbuf, recvbuf, count, datatype, op, root, comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_REDUCE,
                                     sendbytes,
                                     recvbytes );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_1_0_SYMBOL_PMPI_REDUCE_SCATTER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Reduce_scatter )
/**
 * Declaration of PMPI-symbol for MPI_Reduce_scatter
 */
int
PMPI_Reduce_scatter( SCOREP_MPI_CONST_DECL void* sendbuf,
                     void*                       recvbuf,
                     SCOREP_MPI_CONST_DECL int*  recvcounts,
                     MPI_Datatype                datatype,
                     MPI_Op                      op,
                     MPI_Comm                    comm );

/**
 * Measurement wrapper for MPI_Reduce_scatter
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-1.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Reduce_scatter( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, SCOREP_MPI_CONST_DECL int* recvcounts, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    int            i, sz, me, N, M, count = 0;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Type_size( datatype, &sz );
            PMPI_Comm_rank( comm, &me );
            PMPI_Comm_size( comm, &N );

            for ( i = 0; i < N; i++ )
            {
                count += recvcounts[ i ];
            }

            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                if ( sendbuf == MPI_IN_PLACE )
                {
                    count    -= recvcounts[ me ];
                    sendbytes = ( uint64_t )count * sz;
                    recvbytes = ( uint64_t )( N - 1 ) * recvcounts[ me ] * sz;
                }
                else
                {
                    sendbytes = ( uint64_t )count * sz;
                    recvbytes = ( uint64_t )N * recvcounts[ me ] * sz;
                }
            }
            else
            {
                PMPI_Comm_remote_size( comm, &M );

                sendbytes = ( uint64_t )count * sz;
                recvbytes = ( uint64_t )M * recvcounts[ me ] * sz;
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_SCATTER ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_SCATTER ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Reduce_scatter( sendbuf, recvbuf, recvcounts, datatype, op, comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_REDUCE_SCATTER,
                                     sendbytes,
                                     recvbytes );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_SCATTER ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_SCATTER ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_2_2_SYMBOL_PMPI_REDUCE_SCATTER_BLOCK ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Reduce_scatter_block )
/**
 * Declaration of PMPI-symbol for MPI_Reduce_scatter_block
 */
int
PMPI_Reduce_scatter_block( SCOREP_MPI_CONST_DECL void* sendbuf,
                           void*                       recvbuf,
                           int                         recvcount,
                           MPI_Datatype                datatype,
                           MPI_Op                      op,
                           MPI_Comm                    comm );

/**
 * Measurement wrapper for MPI_Reduce_scatter_block
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-2.2
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Reduce_scatter_block( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, int recvcount, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    int            sz, N;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Type_size( datatype, &sz );
            PMPI_Comm_size( comm, &N );

            if ( sendbuf == MPI_IN_PLACE )
            {
                --N;
            }

            sendbytes = ( uint64_t )N * recvcount * sz;
            recvbytes = sendbytes;


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_SCATTER_BLOCK ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_SCATTER_BLOCK ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Reduce_scatter_block( sendbuf, recvbuf, recvcount, datatype, op, comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_REDUCE_SCATTER_BLOCK,
                                     sendbytes,
                                     recvbytes );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_SCATTER_BLOCK ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_SCATTER_BLOCK ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_1_0_SYMBOL_PMPI_SCAN ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Scan )
/**
 * Declaration of PMPI-symbol for MPI_Scan
 */
int
PMPI_Scan( SCOREP_MPI_CONST_DECL void* sendbuf,
           void*                       recvbuf,
           int                         count,
           MPI_Datatype                datatype,
           MPI_Op                      op,
           MPI_Comm                    comm );

/**
 * Measurement wrapper for MPI_Scan
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-1.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Scan( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    int            sz, me, N;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Type_size( datatype, &sz );
            PMPI_Comm_rank( comm, &me );
            PMPI_Comm_size( comm, &N );

            if ( sendbuf == MPI_IN_PLACE )
            {
                sendbytes = ( uint64_t )( N - me - 1 ) * count * sz;
                recvbytes = ( uint64_t )me * count * sz;
            }
            else
            {
                sendbytes = ( uint64_t )( N - me ) * count * sz;
                recvbytes = ( uint64_t )( me + 1 ) * count * sz;
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCAN ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCAN ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Scan( sendbuf, recvbuf, count, datatype, op, comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_SCAN,
                                     sendbytes,
                                     recvbytes );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCAN ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCAN ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_1_0_SYMBOL_PMPI_SCATTER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Scatter )
/**
 * Declaration of PMPI-symbol for MPI_Scatter
 */
int
PMPI_Scatter( SCOREP_MPI_CONST_DECL void* sendbuf,
              int                         sendcount,
              MPI_Datatype                sendtype,
              void*                       recvbuf,
              int                         recvcount,
              MPI_Datatype                recvtype,
              int                         root,
              MPI_Comm                    comm );

/**
 * Measurement wrapper for MPI_Scatter
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-1.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Scatter( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    int            sendsz, recvsz, N, me;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = scorep_mpi_get_scorep_mpi_rank( root );


    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                if ( recvbuf != MPI_IN_PLACE )
                {
                    PMPI_Comm_rank( comm, &me );
                    if ( me == root )
                    {
                        PMPI_Comm_size( comm, &N );
                        PMPI_Type_size( sendtype, &sendsz );
                        sendbytes = ( uint64_t )N * sendcount * sendsz;
                    }

                    PMPI_Type_size( recvtype, &recvsz );
                    recvbytes = ( uint64_t )recvcount * recvsz;
                }
                else
                {
                    PMPI_Comm_size( comm, &N );
                    PMPI_Type_size( sendtype, &sendsz );
                    sendbytes = ( uint64_t )( N - 1 ) * sendcount * sendsz;
                    /* recvbytes is initialized to 0 */
                }
            }
            else
            {
                if ( root == MPI_ROOT )
                {
                    PMPI_Comm_remote_size( comm, &N );
                    PMPI_Type_size( sendtype, &sendsz );
                    sendbytes = ( uint64_t )N * sendcount * sendsz;
                }
                else if ( root != MPI_PROC_NULL )
                {
                    PMPI_Type_size( recvtype, &recvsz );
                    recvbytes = ( uint64_t )recvcount * recvsz;
                }
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCATTER ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCATTER ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Scatter( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_SCATTER,
                                     sendbytes,
                                     recvbytes );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCATTER ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCATTER ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_1_0_SYMBOL_PMPI_SCATTERV ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Scatterv )
/**
 * Declaration of PMPI-symbol for MPI_Scatterv
 */
int
PMPI_Scatterv( SCOREP_MPI_CONST_DECL void* sendbuf,
               SCOREP_MPI_CONST_DECL int*  sendcounts,
               SCOREP_MPI_CONST_DECL int*  displs,
               MPI_Datatype                sendtype,
               void*                       recvbuf,
               int                         recvcount,
               MPI_Datatype                recvtype,
               int                         root,
               MPI_Comm                    comm );

/**
 * Measurement wrapper for MPI_Scatterv
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Coll.w
 * @note C interface
 * @note Introduced with MPI-1.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Scatterv( SCOREP_MPI_CONST_DECL void* sendbuf, SCOREP_MPI_CONST_DECL int* sendcounts, SCOREP_MPI_CONST_DECL int* displs, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    int            sendcount = 0, sendsz = 0, recvsz, me, N, i;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = scorep_mpi_get_scorep_mpi_rank( root );


    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                if ( recvbuf != MPI_IN_PLACE )
                {
                    PMPI_Type_size( recvtype, &recvsz );
                    recvbytes = ( uint64_t )recvcount * recvsz;
                }
                /* MPI_IN_PLACE: recvbytes is initialized to 0 */

                PMPI_Comm_rank( comm, &me );
                if ( me == root )
                {
                    PMPI_Comm_size( comm, &N );
                    PMPI_Type_size( sendtype, &sendsz );
                    for ( i = 0; i < N; i++ )
                    {
                        sendcount += sendcounts[ i ];
                    }

                    if ( recvbuf == MPI_IN_PLACE )
                    {
                        sendcount -= sendcounts[ me ];
                    }
                }
                sendbytes = ( uint64_t )sendcount * sendsz;
            }
            else
            {
                if ( root == MPI_ROOT )
                {
                    PMPI_Comm_remote_size( comm, &N );
                    PMPI_Type_size( sendtype, &sendsz );

                    for ( i = 0; i < N; i++ )
                    {
                        sendcount += sendcounts[ i ];
                    }
                    sendbytes = ( uint64_t )sendcount * sendsz;
                }
                else if ( root != MPI_PROC_NULL )
                {
                    PMPI_Type_size( recvtype, &recvsz );
                    recvbytes = ( uint64_t )recvcount * recvsz;
                }
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCATTERV ] );
            SCOREP_MpiCollectiveBegin();
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCATTERV ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Scatterv( sendbuf, sendcounts, displs, sendtype, recvbuf, recvcount, recvtype, root, comm );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_MpiCollectiveEnd( SCOREP_MPI_COMM_HANDLE( comm ),
                                     root_loc,
                                     SCOREP_MPI_COLLECTIVE__MPI_SCATTERV,
                                     sendbytes,
                                     recvbytes );
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCATTERV ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCATTERV ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif

#if HAVE( MPI_3_0_SYMBOL_PMPI_IALLGATHER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Iallgather )
/**
 * Declaration of PMPI-symbol for MPI_Iallgather
 */
int
PMPI_Iallgather( SCOREP_MPI_CONST_DECL void* sendbuf,
                 int                         sendcount,
                 MPI_Datatype                sendtype,
                 void*                       recvbuf,
                 int                         recvcount,
                 MPI_Datatype                recvtype,
                 MPI_Comm                    comm,
                 MPI_Request*                request );

/**
 * Measurement wrapper for MPI_Iallgather
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Iallgather( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;

    int            recvsz, sendsz, N, M;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Comm_size( comm, &N );
            PMPI_Type_size( recvtype, &recvsz );

            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                if ( sendbuf != MPI_IN_PLACE )
                {
                    PMPI_Type_size( sendtype, &sendsz );
                    sendbytes = ( uint64_t )N * sendcount * sendsz;
                    recvbytes = ( uint64_t )N * recvcount * recvsz;
                }
                else
                {
                    sendbytes = recvbytes = ( uint64_t )( N - 1 ) * recvcount * recvsz;
                }
            }
            else
            {
                PMPI_Comm_remote_size( comm, &M );
                PMPI_Type_size( sendtype, &sendsz );

                sendbytes = ( uint64_t )M * sendcount * sendsz;
                recvbytes = ( uint64_t )M * recvcount * recvsz;
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLGATHER ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLGATHER ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Iallgather( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_IALLGATHER,
                                                 root_loc, sendbytes, recvbytes, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLGATHER ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLGATHER ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_3_0_SYMBOL_PMPI_IALLGATHERV ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Iallgatherv )
/**
 * Declaration of PMPI-symbol for MPI_Iallgatherv
 */
int
PMPI_Iallgatherv( SCOREP_MPI_CONST_DECL void* sendbuf,
                  int                         sendcount,
                  MPI_Datatype                sendtype,
                  void*                       recvbuf,
                  SCOREP_MPI_CONST_DECL int*  recvcounts,
                  SCOREP_MPI_CONST_DECL int*  displs,
                  MPI_Datatype                recvtype,
                  MPI_Comm                    comm,
                  MPI_Request*                request );

/**
 * Measurement wrapper for MPI_Iallgatherv
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Iallgatherv( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, SCOREP_MPI_CONST_DECL int* recvcounts, SCOREP_MPI_CONST_DECL int* displs, MPI_Datatype recvtype, MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;

    int32_t        recvcount = 0, recvsz, sendsz, i, N, M, me;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Comm_size( comm, &N );
            PMPI_Type_size( recvtype, &recvsz );

            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                PMPI_Comm_rank( comm, &me );

                if ( sendbuf != MPI_IN_PLACE )
                {
                    PMPI_Type_size( sendtype, &sendsz );
                    sendbytes = ( uint64_t )N * sendcount * sendsz;
                }
                else
                {
                    sendbytes = ( uint64_t )( N - 1 ) * recvcounts[ me ] * recvsz;
                }

                for ( i = 0; i < N; i++ )
                {
                    recvcount += recvcounts[ i ];
                }

                if ( sendbuf == MPI_IN_PLACE )
                {
                    recvcount -= recvcounts[ me ];
                }
                recvbytes = ( uint64_t )recvcount * recvsz;
            }
            else
            {
                PMPI_Comm_remote_size( comm, &M );
                PMPI_Type_size( sendtype, &sendsz );

                for ( i = 0; i < M; i++ )
                {
                    recvcount += recvcounts[ i ];
                }

                sendbytes = ( uint64_t )M * sendcount * sendsz;
                recvbytes = ( uint64_t )recvcount * recvsz;
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLGATHERV ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLGATHERV ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Iallgatherv( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_IALLGATHERV,
                                                 root_loc, sendbytes, recvbytes, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLGATHERV ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLGATHERV ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_3_0_SYMBOL_PMPI_IALLREDUCE ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Iallreduce )
/**
 * Declaration of PMPI-symbol for MPI_Iallreduce
 */
int
PMPI_Iallreduce( SCOREP_MPI_CONST_DECL void* sendbuf,
                 void*                       recvbuf,
                 int                         count,
                 MPI_Datatype                datatype,
                 MPI_Op                      op,
                 MPI_Comm                    comm,
                 MPI_Request*                request );

/**
 * Measurement wrapper for MPI_Iallreduce
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Iallreduce( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;

    int32_t        sz, N;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Type_size( datatype, &sz );

            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                PMPI_Comm_size( comm, &N );
                if ( sendbuf == MPI_IN_PLACE )
                {
                    N--;
                }
            }
            else
            {
                PMPI_Comm_remote_size( comm, &N );
            }

            sendbytes = recvbytes = ( uint64_t )N * count * sz;


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLREDUCE ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLREDUCE ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Iallreduce( sendbuf, recvbuf, count, datatype, op, comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_IALLREDUCE,
                                                 root_loc, sendbytes, recvbytes, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLREDUCE ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLREDUCE ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_3_0_SYMBOL_PMPI_IALLTOALL ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Ialltoall )
/**
 * Declaration of PMPI-symbol for MPI_Ialltoall
 */
int
PMPI_Ialltoall( SCOREP_MPI_CONST_DECL void* sendbuf,
                int                         sendcount,
                MPI_Datatype                sendtype,
                void*                       recvbuf,
                int                         recvcount,
                MPI_Datatype                recvtype,
                MPI_Comm                    comm,
                MPI_Request*                request );

/**
 * Measurement wrapper for MPI_Ialltoall
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Ialltoall( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;

    int32_t        recvsz, sendsz, N;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                PMPI_Type_size( recvtype, &recvsz );
                PMPI_Comm_size( comm, &N );

                if ( sendbuf == MPI_IN_PLACE )
                {
                    --N;
                }

                sendbytes = ( uint64_t )N * recvcount * recvsz;
                recvbytes = sendbytes;
            }
            else
            {
                PMPI_Comm_remote_size( comm, &N );
                PMPI_Type_size( sendtype, &sendsz );
                PMPI_Type_size( recvtype, &recvsz );

                sendbytes = ( uint64_t )N * sendcount * sendsz;
                recvbytes = ( uint64_t )N * recvcount * recvsz;
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLTOALL ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLTOALL ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Ialltoall( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_IALLTOALL,
                                                 root_loc, sendbytes, recvbytes, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLTOALL ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLTOALL ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_3_0_SYMBOL_PMPI_IALLTOALLV ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Ialltoallv )
/**
 * Declaration of PMPI-symbol for MPI_Ialltoallv
 */
int
PMPI_Ialltoallv( SCOREP_MPI_CONST_DECL void* sendbuf,
                 SCOREP_MPI_CONST_DECL int*  sendcounts,
                 SCOREP_MPI_CONST_DECL int*  sdispls,
                 MPI_Datatype                sendtype,
                 void*                       recvbuf,
                 SCOREP_MPI_CONST_DECL int*  recvcounts,
                 SCOREP_MPI_CONST_DECL int*  rdispls,
                 MPI_Datatype                recvtype,
                 MPI_Comm                    comm,
                 MPI_Request*                request );

/**
 * Measurement wrapper for MPI_Ialltoallv
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Ialltoallv( SCOREP_MPI_CONST_DECL void* sendbuf, SCOREP_MPI_CONST_DECL int* sendcounts, SCOREP_MPI_CONST_DECL int* sdispls, MPI_Datatype sendtype, void* recvbuf, SCOREP_MPI_CONST_DECL int* recvcounts, SCOREP_MPI_CONST_DECL int* rdispls, MPI_Datatype recvtype, MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;

    int            recvcount = 0, recvsz, sendsz, N, i, me;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                PMPI_Comm_size( comm, &N );
                PMPI_Type_size( recvtype, &recvsz );

                if ( sendbuf != MPI_IN_PLACE )
                {
                    PMPI_Type_size( sendtype, &sendsz );
                    for ( i = 0; i < N; i++ )
                    {
                        recvbytes += ( uint64_t )recvcounts[ i ] * recvsz;
                        sendbytes += ( uint64_t )sendcounts[ i ] * sendsz;
                    }
                }
                else
                {
                    PMPI_Comm_rank( comm, &me );
                    for ( i = 0; i < N; i++ )
                    {
                        recvcount += recvcounts[ i ];
                    }

                    recvcount -= recvcounts[ me ];

                    sendbytes = recvbytes = ( uint64_t )recvcount * recvsz;
                }
            }
            else
            {
                PMPI_Comm_remote_size( comm, &N );
                PMPI_Type_size( recvtype, &recvsz );
                PMPI_Type_size( sendtype, &sendsz );

                for ( i = 0; i < N; i++ )
                {
                    recvbytes += ( uint64_t )recvcounts[ i ] * recvsz;
                    sendbytes += ( uint64_t )sendcounts[ i ] * sendsz;
                }
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLTOALLV ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLTOALLV ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Ialltoallv( sendbuf, sendcounts, sdispls, sendtype, recvbuf, recvcounts, rdispls, recvtype, comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_IALLTOALLV,
                                                 root_loc, sendbytes, recvbytes, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLTOALLV ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLTOALLV ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_3_0_SYMBOL_PMPI_IALLTOALLW ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Ialltoallw )
/**
 * Declaration of PMPI-symbol for MPI_Ialltoallw
 */
int
PMPI_Ialltoallw( SCOREP_MPI_CONST_DECL void*        sendbuf,
                 SCOREP_MPI_CONST_DECL int          sendcounts[],
                 SCOREP_MPI_CONST_DECL int          sdispls[],
                 SCOREP_MPI_CONST_DECL MPI_Datatype sendtypes[],
                 void*                              recvbuf,
                 SCOREP_MPI_CONST_DECL int          recvcounts[],
                 SCOREP_MPI_CONST_DECL int          rdispls[],
                 SCOREP_MPI_CONST_DECL MPI_Datatype recvtypes[],
                 MPI_Comm                           comm,
                 MPI_Request*                       request );

/**
 * Measurement wrapper for MPI_Ialltoallw
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Ialltoallw( SCOREP_MPI_CONST_DECL void* sendbuf, SCOREP_MPI_CONST_DECL int sendcounts[], SCOREP_MPI_CONST_DECL int sdispls[], SCOREP_MPI_CONST_DECL MPI_Datatype sendtypes[], void* recvbuf, SCOREP_MPI_CONST_DECL int recvcounts[], SCOREP_MPI_CONST_DECL int rdispls[], SCOREP_MPI_CONST_DECL MPI_Datatype recvtypes[], MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;

    int            recvsz, sendsz, N, i, me;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                PMPI_Comm_size( comm, &N );

                if ( sendbuf != MPI_IN_PLACE )
                {
                    for ( i = 0; i < N; i++ )
                    {
                        PMPI_Type_size( recvtypes[ i ], &recvsz );
                        recvbytes += ( uint64_t )recvcounts[ i ] * recvsz;

                        PMPI_Type_size( sendtypes[ i ], &sendsz );
                        sendbytes += ( uint64_t )sendcounts[ i ] * sendsz;
                    }
                }
                else
                {
                    PMPI_Comm_rank( comm, &me );

                    for ( i = 0; i < N; i++ )
                    {
                        PMPI_Type_size( recvtypes[ i ], &recvsz );
                        recvbytes += ( uint64_t )recvcounts[ i ] * recvsz;
                    }

                    PMPI_Type_size( recvtypes[ me ], &recvsz );
                    recvbytes -= ( uint64_t )recvcounts[ me ] * recvsz;

                    sendbytes = recvbytes;
                }
            }
            else
            {
                PMPI_Comm_remote_size( comm, &N );

                for ( i = 0; i < N; i++ )
                {
                    PMPI_Type_size( recvtypes[ i ], &recvsz );
                    PMPI_Type_size( sendtypes[ i ], &sendsz );
                    recvbytes += ( uint64_t )recvcounts[ i ] * recvsz;
                    sendbytes += ( uint64_t )sendcounts[ i ] * sendsz;
                }
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLTOALLW ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLTOALLW ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Ialltoallw( sendbuf, sendcounts, sdispls, sendtypes, recvbuf, recvcounts, rdispls, recvtypes, comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_IALLTOALLW,
                                                 root_loc, sendbytes, recvbytes, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLTOALLW ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IALLTOALLW ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_3_0_SYMBOL_PMPI_IBARRIER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Ibarrier )
/**
 * Declaration of PMPI-symbol for MPI_Ibarrier
 */
int
PMPI_Ibarrier( MPI_Comm     comm,
               MPI_Request* request );

/**
 * Measurement wrapper for MPI_Ibarrier
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Ibarrier( MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;
    SCOREP_MpiRank      root_loc = SCOREP_INVALID_ROOT_RANK;

    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IBARRIER ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IBARRIER ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Ibarrier( comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_IBARRIER,
                                                 root_loc, 0, 0, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IBARRIER ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IBARRIER ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_3_0_SYMBOL_PMPI_IBCAST ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Ibcast )
/**
 * Declaration of PMPI-symbol for MPI_Ibcast
 */
int
PMPI_Ibcast( void*        buffer,
             int          count,
             MPI_Datatype datatype,
             int          root,
             MPI_Comm     comm,
             MPI_Request* request );

/**
 * Measurement wrapper for MPI_Ibcast
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Ibcast( void* buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;

    int32_t        sz, N, me;
    int32_t        participant = 1;
    uint64_t       sendbytes   = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc    = scorep_mpi_get_scorep_mpi_rank( root );


    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Type_size( datatype, &sz );

            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                PMPI_Comm_rank( comm, &me );
                if ( me == root )
                {
                    PMPI_Comm_size( comm, &N );
                }
                else
                {
                    N = 0;
                }
            }
            else
            {
                if ( root == MPI_ROOT )
                {
                    PMPI_Comm_remote_size( comm, &N );
                    participant = 0;
                }
                else if ( root == MPI_PROC_NULL )
                {
                    participant = 0;
                    N           = 0;
                }
                else
                {
                    N = 0;
                }
            }

            sendbytes = ( uint64_t )N * count * sz;
            recvbytes = ( uint64_t )participant * count * sz;


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IBCAST ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IBCAST ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Ibcast( buffer, count, datatype, root, comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_IBCAST,
                                                 root_loc, sendbytes, recvbytes, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IBCAST ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IBCAST ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_3_0_SYMBOL_PMPI_IEXSCAN ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Iexscan )
/**
 * Declaration of PMPI-symbol for MPI_Iexscan
 */
int
PMPI_Iexscan( SCOREP_MPI_CONST_DECL void* sendbuf,
              void*                       recvbuf,
              int                         count,
              MPI_Datatype                datatype,
              MPI_Op                      op,
              MPI_Comm                    comm,
              MPI_Request*                request );

/**
 * Measurement wrapper for MPI_Iexscan
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Iexscan( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;

    int32_t        sz, me, N;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Type_size( datatype, &sz );
            PMPI_Comm_rank( comm, &me );
            PMPI_Comm_size( comm, &N );

            sendbytes = ( uint64_t )( N - me - 1 ) * sz * count;
            recvbytes = ( uint64_t )me * sz * count;


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IEXSCAN ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IEXSCAN ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Iexscan( sendbuf, recvbuf, count, datatype, op, comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_IEXSCAN,
                                                 root_loc, sendbytes, recvbytes, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IEXSCAN ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IEXSCAN ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_3_0_SYMBOL_PMPI_IGATHER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Igather )
/**
 * Declaration of PMPI-symbol for MPI_Igather
 */
int
PMPI_Igather( SCOREP_MPI_CONST_DECL void* sendbuf,
              int                         sendcount,
              MPI_Datatype                sendtype,
              void*                       recvbuf,
              int                         recvcount,
              MPI_Datatype                recvtype,
              int                         root,
              MPI_Comm                    comm,
              MPI_Request*                request );

/**
 * Measurement wrapper for MPI_Igather
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Igather( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;

    int            sendsz, recvsz, N, me;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = scorep_mpi_get_scorep_mpi_rank( root );


    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                if ( sendbuf != MPI_IN_PLACE )
                {
                    PMPI_Type_size( sendtype, &sendsz );
                    sendbytes = ( uint64_t )sendcount * sendsz;
                }
                /* MPI_IN_PLACE: sendbytes is initialized to 0 */

                PMPI_Comm_rank( comm, &me );
                if ( me == root )
                {
                    PMPI_Comm_size( comm, &N );
                    PMPI_Type_size( recvtype, &recvsz );
                    if ( sendbuf == MPI_IN_PLACE )
                    {
                        --N;
                    }
                    recvbytes = ( uint64_t )N * recvcount * recvsz;
                }
            }
            else
            {
                if ( root == MPI_ROOT )
                {
                    PMPI_Comm_remote_size( comm, &N );
                    PMPI_Type_size( recvtype, &recvsz );
                    recvbytes = ( uint64_t )N * recvcount * recvsz;
                }
                else if ( root != MPI_PROC_NULL )
                {
                    PMPI_Type_size( sendtype, &sendsz );
                    sendbytes = ( uint64_t )sendcount * sendsz;
                }
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IGATHER ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IGATHER ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Igather( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_IGATHER,
                                                 root_loc, sendbytes, recvbytes, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IGATHER ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IGATHER ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_3_0_SYMBOL_PMPI_IGATHERV ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Igatherv )
/**
 * Declaration of PMPI-symbol for MPI_Igatherv
 */
int
PMPI_Igatherv( SCOREP_MPI_CONST_DECL void* sendbuf,
               int                         sendcount,
               MPI_Datatype                sendtype,
               void*                       recvbuf,
               SCOREP_MPI_CONST_DECL int*  recvcounts,
               SCOREP_MPI_CONST_DECL int*  displs,
               MPI_Datatype                recvtype,
               int                         root,
               MPI_Comm                    comm,
               MPI_Request*                request );

/**
 * Measurement wrapper for MPI_Igatherv
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Igatherv( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, SCOREP_MPI_CONST_DECL int* recvcounts, SCOREP_MPI_CONST_DECL int* displs, MPI_Datatype recvtype, int root, MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;

    int            recvsz, sendsz, me, N, i;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = scorep_mpi_get_scorep_mpi_rank( root );


    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                if ( sendbuf != MPI_IN_PLACE )
                {
                    PMPI_Type_size( sendtype, &sendsz );
                    sendbytes = ( uint64_t )sendcount * sendsz;
                }
                /* MPI_IN_PLACE: sendbytes is initialized to 0 */

                PMPI_Comm_rank( comm, &me );
                if ( me == root )
                {
                    PMPI_Comm_size( comm, &N );
                    PMPI_Type_size( recvtype, &recvsz );

                    for ( i = 0; i < N; ++i )
                    {
                        recvbytes += ( uint64_t )recvcounts[ i ] * recvsz;
                    }

                    if ( sendbuf == MPI_IN_PLACE )
                    {
                        recvbytes -= ( uint64_t )recvcounts[ me ] * recvsz;
                    }
                }
            }
            else
            {
                if ( root == MPI_ROOT )
                {
                    PMPI_Comm_remote_size( comm, &N );
                    PMPI_Type_size( recvtype, &recvsz );

                    for ( i = 0; i < N; ++i )
                    {
                        recvbytes += ( uint64_t )recvcounts[ i ] * recvsz;
                    }
                }
                else if ( root != MPI_PROC_NULL )
                {
                    PMPI_Type_size( sendtype, &sendsz );
                    sendbytes = ( uint64_t )sendcount * sendsz;
                }
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IGATHERV ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IGATHERV ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Igatherv( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, root, comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_IGATHERV,
                                                 root_loc, sendbytes, recvbytes, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IGATHERV ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IGATHERV ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_3_0_SYMBOL_PMPI_IREDUCE ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Ireduce )
/**
 * Declaration of PMPI-symbol for MPI_Ireduce
 */
int
PMPI_Ireduce( SCOREP_MPI_CONST_DECL void* sendbuf,
              void*                       recvbuf,
              int                         count,
              MPI_Datatype                datatype,
              MPI_Op                      op,
              int                         root,
              MPI_Comm                    comm,
              MPI_Request*                request );

/**
 * Measurement wrapper for MPI_Ireduce
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Ireduce( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;

    int            sz, me, N;
    uint64_t       sendbytes = 0, recvbytes = 0, participant = 1;
    SCOREP_MpiRank root_loc  = scorep_mpi_get_scorep_mpi_rank( root );


    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Type_size( datatype, &sz );
            PMPI_Comm_rank( comm, &me );
            PMPI_Comm_size( comm, &N );

            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                if ( sendbuf != MPI_IN_PLACE )
                {
                    sendbytes = ( uint64_t )count * sz;
                }
                else
                {
                    --N;
                }

                if ( root == me )
                {
                    recvbytes = ( uint64_t )N * count * sz;
                }
            }
            else
            {
                if ( root == MPI_ROOT )
                {
                    PMPI_Comm_remote_size( comm, &N );
                    recvbytes = ( uint64_t )N * count * sz;
                }
                else if ( root != MPI_PROC_NULL )
                {
                    sendbytes = ( uint64_t )count * sz;
                }
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IREDUCE ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IREDUCE ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Ireduce( sendbuf, recvbuf, count, datatype, op, root, comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_IREDUCE,
                                                 root_loc, sendbytes, recvbytes, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IREDUCE ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IREDUCE ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_3_0_SYMBOL_PMPI_IREDUCE_SCATTER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Ireduce_scatter )
/**
 * Declaration of PMPI-symbol for MPI_Ireduce_scatter
 */
int
PMPI_Ireduce_scatter( SCOREP_MPI_CONST_DECL void* sendbuf,
                      void*                       recvbuf,
                      SCOREP_MPI_CONST_DECL int*  recvcounts,
                      MPI_Datatype                datatype,
                      MPI_Op                      op,
                      MPI_Comm                    comm,
                      MPI_Request*                request );

/**
 * Measurement wrapper for MPI_Ireduce_scatter
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Ireduce_scatter( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, SCOREP_MPI_CONST_DECL int* recvcounts, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;

    int            i, sz, me, N, M, count = 0;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Type_size( datatype, &sz );
            PMPI_Comm_rank( comm, &me );
            PMPI_Comm_size( comm, &N );

            for ( i = 0; i < N; i++ )
            {
                count += recvcounts[ i ];
            }

            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                if ( sendbuf == MPI_IN_PLACE )
                {
                    count    -= recvcounts[ me ];
                    sendbytes = ( uint64_t )count * sz;
                    recvbytes = ( uint64_t )( N - 1 ) * recvcounts[ me ] * sz;
                }
                else
                {
                    sendbytes = ( uint64_t )count * sz;
                    recvbytes = ( uint64_t )N * recvcounts[ me ] * sz;
                }
            }
            else
            {
                PMPI_Comm_remote_size( comm, &M );

                sendbytes = ( uint64_t )count * sz;
                recvbytes = ( uint64_t )M * recvcounts[ me ] * sz;
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IREDUCE_SCATTER ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IREDUCE_SCATTER ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Ireduce_scatter( sendbuf, recvbuf, recvcounts, datatype, op, comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_IREDUCE_SCATTER,
                                                 root_loc, sendbytes, recvbytes, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IREDUCE_SCATTER ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IREDUCE_SCATTER ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_3_0_SYMBOL_PMPI_IREDUCE_SCATTER_BLOCK ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Ireduce_scatter_block )
/**
 * Declaration of PMPI-symbol for MPI_Ireduce_scatter_block
 */
int
PMPI_Ireduce_scatter_block( SCOREP_MPI_CONST_DECL void* sendbuf,
                            void*                       recvbuf,
                            int                         recvcount,
                            MPI_Datatype                datatype,
                            MPI_Op                      op,
                            MPI_Comm                    comm,
                            MPI_Request*                request );

/**
 * Measurement wrapper for MPI_Ireduce_scatter_block
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Ireduce_scatter_block( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, int recvcount, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;

    int            sz, N;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Type_size( datatype, &sz );
            PMPI_Comm_size( comm, &N );

            if ( sendbuf == MPI_IN_PLACE )
            {
                --N;
            }

            sendbytes = ( uint64_t )N * recvcount * sz;
            recvbytes = sendbytes;


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IREDUCE_SCATTER_BLOCK ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IREDUCE_SCATTER_BLOCK ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Ireduce_scatter_block( sendbuf, recvbuf, recvcount, datatype, op, comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_IREDUCE_SCATTER_BLOCK,
                                                 root_loc, sendbytes, recvbytes, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IREDUCE_SCATTER_BLOCK ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_IREDUCE_SCATTER_BLOCK ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_3_0_SYMBOL_PMPI_ISCAN ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Iscan )
/**
 * Declaration of PMPI-symbol for MPI_Iscan
 */
int
PMPI_Iscan( SCOREP_MPI_CONST_DECL void* sendbuf,
            void*                       recvbuf,
            int                         count,
            MPI_Datatype                datatype,
            MPI_Op                      op,
            MPI_Comm                    comm,
            MPI_Request*                request );

/**
 * Measurement wrapper for MPI_Iscan
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Iscan( SCOREP_MPI_CONST_DECL void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;

    int            sz, me, N;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = SCOREP_INVALID_ROOT_RANK;


    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            PMPI_Type_size( datatype, &sz );
            PMPI_Comm_rank( comm, &me );
            PMPI_Comm_size( comm, &N );

            if ( sendbuf == MPI_IN_PLACE )
            {
                sendbytes = ( uint64_t )( N - me - 1 ) * count * sz;
                recvbytes = ( uint64_t )me * count * sz;
            }
            else
            {
                sendbytes = ( uint64_t )( N - me ) * count * sz;
                recvbytes = ( uint64_t )( me + 1 ) * count * sz;
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ISCAN ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ISCAN ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Iscan( sendbuf, recvbuf, count, datatype, op, comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_ISCAN,
                                                 root_loc, sendbytes, recvbytes, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ISCAN ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ISCAN ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_3_0_SYMBOL_PMPI_ISCATTER ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Iscatter )
/**
 * Declaration of PMPI-symbol for MPI_Iscatter
 */
int
PMPI_Iscatter( SCOREP_MPI_CONST_DECL void* sendbuf,
               int                         sendcount,
               MPI_Datatype                sendtype,
               void*                       recvbuf,
               int                         recvcount,
               MPI_Datatype                recvtype,
               int                         root,
               MPI_Comm                    comm,
               MPI_Request*                request );

/**
 * Measurement wrapper for MPI_Iscatter
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Iscatter( SCOREP_MPI_CONST_DECL void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;

    int            sendsz, recvsz, N, me;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = scorep_mpi_get_scorep_mpi_rank( root );


    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                if ( recvbuf != MPI_IN_PLACE )
                {
                    PMPI_Comm_rank( comm, &me );
                    if ( me == root )
                    {
                        PMPI_Comm_size( comm, &N );
                        PMPI_Type_size( sendtype, &sendsz );
                        sendbytes = ( uint64_t )N * sendcount * sendsz;
                    }

                    PMPI_Type_size( recvtype, &recvsz );
                    recvbytes = ( uint64_t )recvcount * recvsz;
                }
                else
                {
                    PMPI_Comm_size( comm, &N );
                    PMPI_Type_size( sendtype, &sendsz );
                    sendbytes = ( uint64_t )( N - 1 ) * sendcount * sendsz;
                    /* recvbytes is initialized to 0 */
                }
            }
            else
            {
                if ( root == MPI_ROOT )
                {
                    PMPI_Comm_remote_size( comm, &N );
                    PMPI_Type_size( sendtype, &sendsz );
                    sendbytes = ( uint64_t )N * sendcount * sendsz;
                }
                else if ( root != MPI_PROC_NULL )
                {
                    PMPI_Type_size( recvtype, &recvsz );
                    recvbytes = ( uint64_t )recvcount * recvsz;
                }
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ISCATTER ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ISCATTER ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Iscatter( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_ISCATTER,
                                                 root_loc, sendbytes, recvbytes, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ISCATTER ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ISCATTER ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_3_0_SYMBOL_PMPI_ISCATTERV ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Iscatterv )
/**
 * Declaration of PMPI-symbol for MPI_Iscatterv
 */
int
PMPI_Iscatterv( SCOREP_MPI_CONST_DECL void* sendbuf,
                SCOREP_MPI_CONST_DECL int*  sendcounts,
                SCOREP_MPI_CONST_DECL int*  displs,
                MPI_Datatype                sendtype,
                void*                       recvbuf,
                int                         recvcount,
                MPI_Datatype                recvtype,
                int                         root,
                MPI_Comm                    comm,
                MPI_Request*                request );

/**
 * Measurement wrapper for MPI_Iscatterv
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Icoll.w
 * @note C interface
 * @note Introduced with MPI-3.0
 * @ingroup coll
 * It wraps the mpi call with an enter and exit event. Additionally, a collective
 * event is generated in between the enter and exit event after the PMPI call.
 */
int
MPI_Iscatterv( SCOREP_MPI_CONST_DECL void* sendbuf, SCOREP_MPI_CONST_DECL int* sendcounts, SCOREP_MPI_CONST_DECL int* displs, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int           event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int           event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int                 return_val;
    SCOREP_MpiRequestId reqid;

    int            sendcount = 0, sendsz = 0, recvsz, me, N, i;
    uint64_t       sendbytes = 0, recvbytes = 0;
    SCOREP_MpiRank root_loc  = scorep_mpi_get_scorep_mpi_rank( root );


    if ( event_gen_active )
    {
        reqid = scorep_mpi_get_request_id();
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            int is_inter_comm = 0;
            PMPI_Comm_test_inter( comm, &is_inter_comm );
            if ( !is_inter_comm )
            {
                if ( recvbuf != MPI_IN_PLACE )
                {
                    PMPI_Type_size( recvtype, &recvsz );
                    recvbytes = ( uint64_t )recvcount * recvsz;
                }
                /* MPI_IN_PLACE: recvbytes is initialized to 0 */

                PMPI_Comm_rank( comm, &me );
                if ( me == root )
                {
                    PMPI_Comm_size( comm, &N );
                    PMPI_Type_size( sendtype, &sendsz );
                    for ( i = 0; i < N; i++ )
                    {
                        sendcount += sendcounts[ i ];
                    }

                    if ( recvbuf == MPI_IN_PLACE )
                    {
                        sendcount -= sendcounts[ me ];
                    }
                }
                sendbytes = ( uint64_t )sendcount * sendsz;
            }
            else
            {
                if ( root == MPI_ROOT )
                {
                    PMPI_Comm_remote_size( comm, &N );
                    PMPI_Type_size( sendtype, &sendsz );

                    for ( i = 0; i < N; i++ )
                    {
                        sendcount += sendcounts[ i ];
                    }
                    sendbytes = ( uint64_t )sendcount * sendsz;
                }
                else if ( root != MPI_PROC_NULL )
                {
                    PMPI_Type_size( recvtype, &recvsz );
                    recvbytes = ( uint64_t )recvcount * recvsz;
                }
            }


            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ISCATTERV ] );
            SCOREP_MpiNonBlockingCollectiveRequest( reqid );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ISCATTERV ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Iscatterv( sendbuf, sendcounts, displs, sendtype, recvbuf, recvcount, recvtype, root, comm, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            if ( return_val == MPI_SUCCESS )
            {
                scorep_mpi_request_icoll_create( *request, SCOREP_MPI_REQUEST_FLAG_NONE, SCOREP_MPI_COLLECTIVE__MPI_ISCATTERV,
                                                 root_loc, sendbytes, recvbytes, comm, reqid );
            }
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ISCATTERV ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ISCATTERV ] );
        }

        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif

#if HAVE( MPI_4_0_SYMBOL_PMPI_ALLGATHER_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Allgather_init )
/**
 * Declaration of PMPI-symbol for MPI_Allgather_init
 */
int
PMPI_Allgather_init( const void*  sendbuf,
                     int          sendcount,
                     MPI_Datatype sendtype,
                     void*        recvbuf,
                     int          recvcount,
                     MPI_Datatype recvtype,
                     MPI_Comm     comm,
                     MPI_Info     info,
                     MPI_Request* request );

/**
 * Measurement wrapper for MPI_Allgather_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Allgather_init call with enter and exit events.
 */
int
MPI_Allgather_init( const void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLGATHER_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLGATHER_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Allgather_init( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLGATHER_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLGATHER_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_4_0_SYMBOL_PMPI_ALLGATHERV_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Allgatherv_init )
/**
 * Declaration of PMPI-symbol for MPI_Allgatherv_init
 */
int
PMPI_Allgatherv_init( const void*  sendbuf,
                      int          sendcount,
                      MPI_Datatype sendtype,
                      void*        recvbuf,
                      const int    recvcounts[],
                      const int    displs[],
                      MPI_Datatype recvtype,
                      MPI_Comm     comm,
                      MPI_Info     info,
                      MPI_Request* request );

/**
 * Measurement wrapper for MPI_Allgatherv_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Allgatherv_init call with enter and exit events.
 */
int
MPI_Allgatherv_init( const void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, const int recvcounts[], const int displs[], MPI_Datatype recvtype, MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLGATHERV_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLGATHERV_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Allgatherv_init( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLGATHERV_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLGATHERV_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_4_0_SYMBOL_PMPI_ALLREDUCE_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Allreduce_init )
/**
 * Declaration of PMPI-symbol for MPI_Allreduce_init
 */
int
PMPI_Allreduce_init( const void*  sendbuf,
                     void*        recvbuf,
                     int          count,
                     MPI_Datatype datatype,
                     MPI_Op       op,
                     MPI_Comm     comm,
                     MPI_Info     info,
                     MPI_Request* request );

/**
 * Measurement wrapper for MPI_Allreduce_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Allreduce_init call with enter and exit events.
 */
int
MPI_Allreduce_init( const void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLREDUCE_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLREDUCE_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Allreduce_init( sendbuf, recvbuf, count, datatype, op, comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLREDUCE_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLREDUCE_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_4_0_SYMBOL_PMPI_ALLTOALL_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Alltoall_init )
/**
 * Declaration of PMPI-symbol for MPI_Alltoall_init
 */
int
PMPI_Alltoall_init( const void*  sendbuf,
                    int          sendcount,
                    MPI_Datatype sendtype,
                    void*        recvbuf,
                    int          recvcount,
                    MPI_Datatype recvtype,
                    MPI_Comm     comm,
                    MPI_Info     info,
                    MPI_Request* request );

/**
 * Measurement wrapper for MPI_Alltoall_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Alltoall_init call with enter and exit events.
 */
int
MPI_Alltoall_init( const void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALL_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALL_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Alltoall_init( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALL_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALL_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_4_0_SYMBOL_PMPI_ALLTOALLV_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Alltoallv_init )
/**
 * Declaration of PMPI-symbol for MPI_Alltoallv_init
 */
int
PMPI_Alltoallv_init( const void*  sendbuf,
                     const int    sendcounts[],
                     const int    sdispls[],
                     MPI_Datatype sendtype,
                     void*        recvbuf,
                     const int    recvcounts[],
                     const int    rdispls[],
                     MPI_Datatype recvtype,
                     MPI_Comm     comm,
                     MPI_Info     info,
                     MPI_Request* request );

/**
 * Measurement wrapper for MPI_Alltoallv_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Alltoallv_init call with enter and exit events.
 */
int
MPI_Alltoallv_init( const void* sendbuf, const int sendcounts[], const int sdispls[], MPI_Datatype sendtype, void* recvbuf, const int recvcounts[], const int rdispls[], MPI_Datatype recvtype, MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALLV_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALLV_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Alltoallv_init( sendbuf, sendcounts, sdispls, sendtype, recvbuf, recvcounts, rdispls, recvtype, comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALLV_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALLV_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_4_0_SYMBOL_PMPI_ALLTOALLW_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Alltoallw_init )
/**
 * Declaration of PMPI-symbol for MPI_Alltoallw_init
 */
int
PMPI_Alltoallw_init( const void*        sendbuf,
                     const int          sendcounts[],
                     const int          sdispls[],
                     const MPI_Datatype sendtypes[],
                     void*              recvbuf,
                     const int          recvcounts[],
                     const int          rdispls[],
                     const MPI_Datatype recvtypes[],
                     MPI_Comm           comm,
                     MPI_Info           info,
                     MPI_Request*       request );

/**
 * Measurement wrapper for MPI_Alltoallw_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Alltoallw_init call with enter and exit events.
 */
int
MPI_Alltoallw_init( const void* sendbuf, const int sendcounts[], const int sdispls[], const MPI_Datatype sendtypes[], void* recvbuf, const int recvcounts[], const int rdispls[], const MPI_Datatype recvtypes[], MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALLW_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALLW_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Alltoallw_init( sendbuf, sendcounts, sdispls, sendtypes, recvbuf, recvcounts, rdispls, recvtypes, comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALLW_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_ALLTOALLW_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_4_0_SYMBOL_PMPI_BARRIER_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Barrier_init )
/**
 * Declaration of PMPI-symbol for MPI_Barrier_init
 */
int
PMPI_Barrier_init( MPI_Comm     comm,
                   MPI_Info     info,
                   MPI_Request* request );

/**
 * Measurement wrapper for MPI_Barrier_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Barrier_init call with enter and exit events.
 */
int
MPI_Barrier_init( MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_BARRIER_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_BARRIER_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Barrier_init( comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_BARRIER_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_BARRIER_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_4_0_SYMBOL_PMPI_BCAST_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Bcast_init )
/**
 * Declaration of PMPI-symbol for MPI_Bcast_init
 */
int
PMPI_Bcast_init( void*        buffer,
                 int          count,
                 MPI_Datatype datatype,
                 int          root,
                 MPI_Comm     comm,
                 MPI_Info     info,
                 MPI_Request* request );

/**
 * Measurement wrapper for MPI_Bcast_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Bcast_init call with enter and exit events.
 */
int
MPI_Bcast_init( void* buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_BCAST_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_BCAST_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Bcast_init( buffer, count, datatype, root, comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_BCAST_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_BCAST_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_4_0_SYMBOL_PMPI_EXSCAN_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Exscan_init )
/**
 * Declaration of PMPI-symbol for MPI_Exscan_init
 */
int
PMPI_Exscan_init( const void*  sendbuf,
                  void*        recvbuf,
                  int          count,
                  MPI_Datatype datatype,
                  MPI_Op       op,
                  MPI_Comm     comm,
                  MPI_Info     info,
                  MPI_Request* request );

/**
 * Measurement wrapper for MPI_Exscan_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Exscan_init call with enter and exit events.
 */
int
MPI_Exscan_init( const void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_EXSCAN_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_EXSCAN_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Exscan_init( sendbuf, recvbuf, count, datatype, op, comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_EXSCAN_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_EXSCAN_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_4_0_SYMBOL_PMPI_GATHER_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Gather_init )
/**
 * Declaration of PMPI-symbol for MPI_Gather_init
 */
int
PMPI_Gather_init( const void*  sendbuf,
                  int          sendcount,
                  MPI_Datatype sendtype,
                  void*        recvbuf,
                  int          recvcount,
                  MPI_Datatype recvtype,
                  int          root,
                  MPI_Comm     comm,
                  MPI_Info     info,
                  MPI_Request* request );

/**
 * Measurement wrapper for MPI_Gather_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Gather_init call with enter and exit events.
 */
int
MPI_Gather_init( const void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_GATHER_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_GATHER_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Gather_init( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_GATHER_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_GATHER_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_4_0_SYMBOL_PMPI_GATHERV_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Gatherv_init )
/**
 * Declaration of PMPI-symbol for MPI_Gatherv_init
 */
int
PMPI_Gatherv_init( const void*  sendbuf,
                   int          sendcount,
                   MPI_Datatype sendtype,
                   void*        recvbuf,
                   const int    recvcounts[],
                   const int    displs[],
                   MPI_Datatype recvtype,
                   int          root,
                   MPI_Comm     comm,
                   MPI_Info     info,
                   MPI_Request* request );

/**
 * Measurement wrapper for MPI_Gatherv_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Gatherv_init call with enter and exit events.
 */
int
MPI_Gatherv_init( const void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, const int recvcounts[], const int displs[], MPI_Datatype recvtype, int root, MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_GATHERV_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_GATHERV_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Gatherv_init( sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, recvtype, root, comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_GATHERV_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_GATHERV_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_4_0_SYMBOL_PMPI_REDUCE_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Reduce_init )
/**
 * Declaration of PMPI-symbol for MPI_Reduce_init
 */
int
PMPI_Reduce_init( const void*  sendbuf,
                  void*        recvbuf,
                  int          count,
                  MPI_Datatype datatype,
                  MPI_Op       op,
                  int          root,
                  MPI_Comm     comm,
                  MPI_Info     info,
                  MPI_Request* request );

/**
 * Measurement wrapper for MPI_Reduce_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Reduce_init call with enter and exit events.
 */
int
MPI_Reduce_init( const void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Reduce_init( sendbuf, recvbuf, count, datatype, op, root, comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_4_0_SYMBOL_PMPI_REDUCE_SCATTER_BLOCK_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Reduce_scatter_block_init )
/**
 * Declaration of PMPI-symbol for MPI_Reduce_scatter_block_init
 */
int
PMPI_Reduce_scatter_block_init( const void*  sendbuf,
                                void*        recvbuf,
                                int          recvcount,
                                MPI_Datatype datatype,
                                MPI_Op       op,
                                MPI_Comm     comm,
                                MPI_Info     info,
                                MPI_Request* request );

/**
 * Measurement wrapper for MPI_Reduce_scatter_block_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Reduce_scatter_block_init call with enter and exit events.
 */
int
MPI_Reduce_scatter_block_init( const void* sendbuf, void* recvbuf, int recvcount, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_SCATTER_BLOCK_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_SCATTER_BLOCK_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Reduce_scatter_block_init( sendbuf, recvbuf, recvcount, datatype, op, comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_SCATTER_BLOCK_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_SCATTER_BLOCK_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_4_0_SYMBOL_PMPI_REDUCE_SCATTER_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Reduce_scatter_init )
/**
 * Declaration of PMPI-symbol for MPI_Reduce_scatter_init
 */
int
PMPI_Reduce_scatter_init( const void*  sendbuf,
                          void*        recvbuf,
                          const int    recvcounts[],
                          MPI_Datatype datatype,
                          MPI_Op       op,
                          MPI_Comm     comm,
                          MPI_Info     info,
                          MPI_Request* request );

/**
 * Measurement wrapper for MPI_Reduce_scatter_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Reduce_scatter_init call with enter and exit events.
 */
int
MPI_Reduce_scatter_init( const void* sendbuf, void* recvbuf, const int recvcounts[], MPI_Datatype datatype, MPI_Op op, MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_SCATTER_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_SCATTER_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Reduce_scatter_init( sendbuf, recvbuf, recvcounts, datatype, op, comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_SCATTER_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_SCATTER_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_4_0_SYMBOL_PMPI_SCAN_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Scan_init )
/**
 * Declaration of PMPI-symbol for MPI_Scan_init
 */
int
PMPI_Scan_init( const void*  sendbuf,
                void*        recvbuf,
                int          count,
                MPI_Datatype datatype,
                MPI_Op       op,
                MPI_Comm     comm,
                MPI_Info     info,
                MPI_Request* request );

/**
 * Measurement wrapper for MPI_Scan_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Scan_init call with enter and exit events.
 */
int
MPI_Scan_init( const void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCAN_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCAN_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Scan_init( sendbuf, recvbuf, count, datatype, op, comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCAN_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCAN_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_4_0_SYMBOL_PMPI_SCATTER_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Scatter_init )
/**
 * Declaration of PMPI-symbol for MPI_Scatter_init
 */
int
PMPI_Scatter_init( const void*  sendbuf,
                   int          sendcount,
                   MPI_Datatype sendtype,
                   void*        recvbuf,
                   int          recvcount,
                   MPI_Datatype recvtype,
                   int          root,
                   MPI_Comm     comm,
                   MPI_Info     info,
                   MPI_Request* request );

/**
 * Measurement wrapper for MPI_Scatter_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Scatter_init call with enter and exit events.
 */
int
MPI_Scatter_init( const void* sendbuf, int sendcount, MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCATTER_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCATTER_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Scatter_init( sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, root, comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCATTER_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCATTER_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
#if HAVE( MPI_4_0_SYMBOL_PMPI_SCATTERV_INIT ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Scatterv_init )
/**
 * Declaration of PMPI-symbol for MPI_Scatterv_init
 */
int
PMPI_Scatterv_init( const void*  sendbuf,
                    const int    sendcounts[],
                    const int    displs[],
                    MPI_Datatype sendtype,
                    void*        recvbuf,
                    int          recvcount,
                    MPI_Datatype recvtype,
                    int          root,
                    MPI_Comm     comm,
                    MPI_Info     info,
                    MPI_Request* request );

/**
 * Measurement wrapper for MPI_Scatterv_init
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-4.0
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Scatterv_init call with enter and exit events.
 */
int
MPI_Scatterv_init( const void* sendbuf, const int sendcounts[], const int displs[], MPI_Datatype sendtype, void* recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm, MPI_Info info, MPI_Request* request )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCATTERV_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCATTERV_INIT ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Scatterv_init( sendbuf, sendcounts, displs, sendtype, recvbuf, recvcount, recvtype, root, comm, info, request );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCATTERV_INIT ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_SCATTERV_INIT ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif

#if HAVE( MPI_2_2_SYMBOL_PMPI_REDUCE_LOCAL ) && !defined( SCOREP_MPI_NO_COLL ) && !defined( MPI_Reduce_local )
/**
 * Declaration of PMPI-symbol for MPI_Reduce_local
 */
int
PMPI_Reduce_local( SCOREP_MPI_CONST_DECL void* inbuf,
                   void*                       inoutbuf,
                   int                         count,
                   MPI_Datatype                datatype,
                   MPI_Op                      op );

/**
 * Measurement wrapper for MPI_Reduce_local
 * @note Auto-generated by wrapgen from template: SCOREP_Mpi_Std.w
 * @note C interface
 * @note Introduced with MPI-2.2
 * @ingroup coll
 * Triggers an enter and exit event.
 * It wraps the MPI_Reduce_local call with enter and exit events.
 */
int
MPI_Reduce_local( SCOREP_MPI_CONST_DECL void* inbuf, void* inoutbuf, int count, MPI_Datatype datatype, MPI_Op op )
{
    SCOREP_IN_MEASUREMENT_INCREMENT();
    const int event_gen_active           = SCOREP_MPI_IS_EVENT_GEN_ON;
    const int event_gen_active_for_group = SCOREP_MPI_IS_EVENT_GEN_ON_FOR( SCOREP_MPI_ENABLED_COLL );
    int       return_val;

    if ( event_gen_active )
    {
        SCOREP_MPI_EVENT_GEN_OFF();
        if ( event_gen_active_for_group )
        {
            SCOREP_EnterWrappedRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_LOCAL ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_EnterWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_LOCAL ] );
        }
    }

    SCOREP_ENTER_WRAPPED_REGION();
    return_val = PMPI_Reduce_local( inbuf, inoutbuf, count, datatype, op );
    SCOREP_EXIT_WRAPPED_REGION();

    if ( event_gen_active )
    {
        if ( event_gen_active_for_group )
        {
            SCOREP_ExitRegion( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_LOCAL ] );
        }
        else if ( SCOREP_IsUnwindingEnabled() )
        {
            SCOREP_ExitWrapper( scorep_mpi_regions[ SCOREP_MPI_REGION__MPI_REDUCE_LOCAL ] );
        }
        SCOREP_MPI_EVENT_GEN_ON();
    }

    SCOREP_IN_MEASUREMENT_DECREMENT();
    return return_val;
}
#endif
/**
 * @}
 */
